{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read artcile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://www.thushv.com/sequential_modelling/long-short-term-memory-lstm-networks-demystified/\n",
    "# http://www.thushv.com/sequential_modelling/long-short-term-memory-lstm-networks-implementing-with-tensorflow-part-2/    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return tf.compat.as_str(f.read(name))\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchists ad']\n",
      "17\n",
      "['ons anarchists ad']\n"
     ]
    }
   ],
   "source": [
    "batch_size=1\n",
    "num_unrollings=16\n",
    "import pdb\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "#     pdb.set_trace()\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "#     pdb.set_trace()\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "#     pdb.set_trace()\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "#     pdb.set_trace()\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "#     pdb.set_trace()\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "#   pdb.set_trace()\n",
    "\n",
    "  abc =  [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "#   pdb.set_trace()\n",
    "  return abc\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s_1 = [''] * batches[0].shape[0]\n",
    "#   s_1 = [''] * 5\n",
    "#   pdb.set_trace()\n",
    "  for boy in batches:\n",
    "#     pdb.set_trace()\n",
    "    s_1 = [''.join(x) for x in zip(s_1, characters(boy))]\n",
    "#     pdb.set_trace()\n",
    "    len(s_1)\n",
    "#   pdb.set_trace()\n",
    "  return s_1\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "abc = train_batches.next()\n",
    "print(batches2string(abc))\n",
    "print (len(abc))\n",
    "print(batches2string(abc))\n",
    "# print(batches2string(train_batches.next()))\n",
    "# print(batches2string(valid_batches.next()))\n",
    "# print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batches2string(train_batches.next())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-30-43c8b13f6a78>:5 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Average loss at step 0: 3.298519 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.07\n",
      "================================================================================\n",
      "csemetvsejzyfctbbbufhvdkuxdeofahsa sdgs nuass bitvaoaemkvwi gqzkfserpdrverbawixo\n",
      "iwsefbddfd jskt wasmlditojlsnjkkatsnn bmxt pouypjqclleee a oaj lkpamlksb enfdplr\n",
      "qmihvo olsxfoacf u ybmdpysbkbs uapbtx dutfabkt yotatf n a ttznudqfntebajsrn pfua\n",
      "bs lndhixpjpzlybxisgdc xlc dlzsesgn ajsen tpps xssdrtnofsnfauyyqetnfunpkxptoaabb\n",
      "m qa ass ncaedeqa omfaddoxppgaalibeipvlopfdwuaeuyjyoegbnssowktjsbnthtitdax s aab\n",
      "================================================================================\n",
      "Validation set perplexity: 21.58\n",
      "Average loss at step 100: 2.993810 learning rate: 10.000000\n",
      "Minibatch perplexity: 12.33\n",
      "Validation set perplexity: 17.43\n",
      "Average loss at step 200: 2.678977 learning rate: 10.000000\n",
      "Minibatch perplexity: 12.24\n",
      "Validation set perplexity: 15.14\n",
      "Average loss at step 300: 2.679231 learning rate: 10.000000\n",
      "Minibatch perplexity: 16.43\n",
      "Validation set perplexity: 15.98\n",
      "Average loss at step 400: 2.552258 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.21\n",
      "Validation set perplexity: 18.74\n",
      "Average loss at step 500: 2.477425 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.28\n",
      "Validation set perplexity: 11.97\n",
      "Average loss at step 600: 2.431887 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.22\n",
      "Validation set perplexity: 11.82\n",
      "Average loss at step 700: 2.454813 learning rate: 10.000000\n",
      "Minibatch perplexity: 12.07\n",
      "Validation set perplexity: 11.85\n",
      "Average loss at step 800: 2.341319 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.68\n",
      "Validation set perplexity: 14.25\n",
      "Average loss at step 900: 2.250708 learning rate: 10.000000\n",
      "Minibatch perplexity: 12.42\n",
      "Validation set perplexity: 12.93\n",
      "Average loss at step 1000: 2.183143 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.69\n",
      "================================================================================\n",
      "s as diket ussts shubibeishuruks wis opsundisorich oriviy seeduchers sircth igtr\n",
      "x bersgun or insocgunik ak in sporopwiks as or porpirhed sushevicts ys hine wevi\n",
      " oheviriosh shd sevevinatesxigsor shishmssblurevion orssivicts in uche the wever\n",
      "xus worhiuvs pous omijiins in pakedred sovks biveviks ir s und udsevectromind ou\n",
      "krsesgutshed onarchists ry fed the or oneholistsy the wichis oished igtoshed sun\n",
      "================================================================================\n",
      "Validation set perplexity: 10.40\n",
      "Average loss at step 1100: 2.238160 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.80\n",
      "Validation set perplexity: 9.69\n",
      "Average loss at step 1200: 2.232830 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.47\n",
      "Validation set perplexity: 10.94\n",
      "Average loss at step 1300: 2.141264 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.46\n",
      "Validation set perplexity: 8.98\n",
      "Average loss at step 1400: 2.325868 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.95\n",
      "Validation set perplexity: 10.04\n",
      "Average loss at step 1500: 2.194580 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.95\n",
      "Validation set perplexity: 9.43\n",
      "Average loss at step 1600: 2.132139 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.88\n",
      "Validation set perplexity: 9.22\n",
      "Average loss at step 1700: 2.133640 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 9.22\n",
      "Average loss at step 1800: 2.230382 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.81\n",
      "Validation set perplexity: 9.51\n",
      "Average loss at step 1900: 2.238363 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.05\n",
      "Validation set perplexity: 9.28\n",
      "Average loss at step 2000: 2.169282 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.70\n",
      "================================================================================\n",
      "th octicispouses anarchist gybocis plenn phinist pivippphications pigaminnsbiati\n",
      "hasle and inchilssiptvelosistiitisupan sphapany solily inclisbipocalisciinis cit\n",
      "isisiitism is is action in tharipliturmipsince anarchiscing ph pophiity siutiiti\n",
      "nilsins miliphalminveeses phiincirintical ppslispally histiony vollacel citation\n",
      "wcintion psocienes miphe phismentioncicisle hishphihoal phatly fivicist phicical\n",
      "================================================================================\n",
      "Validation set perplexity: 8.65\n",
      "Average loss at step 2100: 2.187907 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.81\n",
      "Validation set perplexity: 8.51\n",
      "Average loss at step 2200: 2.255895 learning rate: 10.000000\n",
      "Minibatch perplexity: 16.45\n",
      "Validation set perplexity: 8.44\n",
      "Average loss at step 2300: 2.139692 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 9.74\n",
      "Average loss at step 2400: 2.254332 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.02\n",
      "Validation set perplexity: 8.18\n",
      "Average loss at step 2500: 2.189011 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.88\n",
      "Validation set perplexity: 10.74\n",
      "Average loss at step 2600: 2.127141 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.73\n",
      "Validation set perplexity: 11.64\n",
      "Average loss at step 2700: 1.984504 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.57\n",
      "Validation set perplexity: 8.65\n",
      "Average loss at step 2800: 2.294605 learning rate: 10.000000\n",
      "Minibatch perplexity: 17.08\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 2900: 2.202211 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 9.84\n",
      "Average loss at step 3000: 2.243421 learning rate: 10.000000\n",
      "Minibatch perplexity: 2.85\n",
      "================================================================================\n",
      "date temm trreeatered the are seughor tre treetarientad man term a they tation a\n",
      "d ne unterk term the treted autiston tweter atterterny tors theur chax theor the\n",
      "s sententation a minion theplus ferm s thatiand wheratere with diam the reurbiso\n",
      "fter thely miteristers thes alth the wore ceminy pugasism whated arterm deores r\n",
      "xterming may a pecury determ the the term are wermmenty the at the they a the am\n",
      "================================================================================\n",
      "Validation set perplexity: 8.17\n",
      "Average loss at step 3100: 2.113722 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 9.03\n",
      "Average loss at step 3200: 2.154672 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.90\n",
      "Validation set perplexity: 9.02\n",
      "Average loss at step 3300: 2.157455 learning rate: 10.000000\n",
      "Minibatch perplexity: 2.08\n",
      "Validation set perplexity: 10.51\n",
      "Average loss at step 3400: 2.143023 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 9.93\n",
      "Average loss at step 3500: 2.009034 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.27\n",
      "Validation set perplexity: 9.09\n",
      "Average loss at step 3600: 1.989214 learning rate: 10.000000\n",
      "Minibatch perplexity: 12.18\n",
      "Validation set perplexity: 9.15\n",
      "Average loss at step 3700: 2.056279 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.22\n",
      "Validation set perplexity: 9.87\n",
      "Average loss at step 3800: 1.988928 learning rate: 10.000000\n",
      "Minibatch perplexity: 18.79\n",
      "Validation set perplexity: 9.07\n",
      "Average loss at step 3900: 2.121386 learning rate: 10.000000\n",
      "Minibatch perplexity: 15.77\n",
      "Validation set perplexity: 10.57\n",
      "Average loss at step 4000: 2.155482 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.02\n",
      "================================================================================\n",
      "ge as and any for dehavive to for reo and repo as awge and as gor imautivivitive\n",
      "s phicic seist coups are begria and as ared thys hapst are a puast are whwian us\n",
      "vefucdiony has an ime and appod as graon ae is at and autistic sativion at lavio\n",
      "khies and and fingussios and are ave in dn dearit of reaghor in to s as vadietiv\n",
      "ral decarlautism gefererdem paod thosed nehaghol those with autistic agg hivivid\n",
      "================================================================================\n",
      "Validation set perplexity: 9.12\n",
      "Average loss at step 4100: 1.995410 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 11.07\n",
      "Average loss at step 4200: 1.914998 learning rate: 10.000000\n",
      "Minibatch perplexity: 2.74\n",
      "Validation set perplexity: 12.18\n",
      "Average loss at step 4300: 2.114414 learning rate: 10.000000\n",
      "Minibatch perplexity: 15.29\n",
      "Validation set perplexity: 12.39\n",
      "Average loss at step 4400: 2.148522 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 9.88\n",
      "Average loss at step 4500: 1.981331 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.34\n",
      "Validation set perplexity: 11.20\n",
      "Average loss at step 4600: 1.986891 learning rate: 10.000000\n",
      "Minibatch perplexity: 2.94\n",
      "Validation set perplexity: 9.69\n",
      "Average loss at step 4700: 1.996748 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 9.46\n",
      "Average loss at step 4800: 1.960866 learning rate: 10.000000\n",
      "Minibatch perplexity: 14.64\n",
      "Validation set perplexity: 11.12\n",
      "Average loss at step 4900: 2.045231 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.58\n",
      "Validation set perplexity: 11.48\n",
      "Average loss at step 5000: 1.857436 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.31\n",
      "================================================================================\n",
      "rantinin alit stovialotiatm diagtin its nt spectresss acimiso spitiveiiomialsts \n",
      "y tosode abk bsitibims tifinguaissivoninentnt finemifithticas eificationmialisvi\n",
      "zeriintes bich measitisionis ontrat antram roubis tifagisifictinaminenfisimianal\n",
      "t inesies aondisinvise alitboistntitaris flenitiiimalin ticces fentlistctibilimi\n",
      "grtofenrtacesiatifitatist bussoroticaim bie nvoandocaiontilinifinichitibingsraip\n",
      "================================================================================\n",
      "Validation set perplexity: 12.16\n",
      "Average loss at step 5100: 1.941889 learning rate: 1.000000\n",
      "Minibatch perplexity: 13.24\n",
      "Validation set perplexity: 8.31\n",
      "Average loss at step 5200: 1.836073 learning rate: 1.000000\n",
      "Minibatch perplexity: 11.21\n",
      "Validation set perplexity: 7.94\n",
      "Average loss at step 5300: 2.053326 learning rate: 1.000000\n",
      "Minibatch perplexity: 10.51\n",
      "Validation set perplexity: 7.72\n",
      "Average loss at step 5400: 2.004181 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 7.68\n",
      "Average loss at step 5500: 1.888212 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 7.67\n",
      "Average loss at step 5600: 1.954324 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.53\n",
      "Validation set perplexity: 7.57\n",
      "Average loss at step 5700: 1.987785 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.26\n",
      "Validation set perplexity: 7.57\n",
      "Average loss at step 5800: 2.068964 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.46\n",
      "Validation set perplexity: 7.38\n",
      "Average loss at step 5900: 2.013296 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 6000: 1.898589 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.10\n",
      "================================================================================\n",
      "orm in and zero revire on the the for the s paikurate mon abilmed istaing chllti\n",
      "ls on the the nwoam oshes t zero whint and sucomed hastssaided drarnil consys ag\n",
      "m arout with rarons apperal ear is khrias plines one wharred ithan apaure hale g\n",
      "a empaict tro saberies zero abil lby st stan al lplaltises vas perace mand surti\n",
      "ved ampaterakbued lend obical lases atsides bus zero pace sine three to one auti\n",
      "================================================================================\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 6100: 1.921423 learning rate: 1.000000\n",
      "Minibatch perplexity: 10.42\n",
      "Validation set perplexity: 7.63\n",
      "Average loss at step 6200: 2.100694 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 7.46\n",
      "Average loss at step 6300: 1.962193 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.94\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 6400: 1.880906 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.63\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 6500: 1.993397 learning rate: 1.000000\n",
      "Minibatch perplexity: 15.38\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 6600: 1.952325 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.83\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 6700: 2.033033 learning rate: 1.000000\n",
      "Minibatch perplexity: 9.94\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 6800: 1.769983 learning rate: 1.000000\n",
      "Minibatch perplexity: 2.81\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 6900: 1.930450 learning rate: 1.000000\n",
      "Minibatch perplexity: 13.39\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 7000: 1.876667 learning rate: 1.000000\n",
      "Minibatch perplexity: 11.34\n",
      "================================================================================\n",
      "qity gens manceured and is saxcical wa one but in grent ribest sevelirp exblevea\n",
      "al comment genlar timf three the uniamerar liame thine urshent wea five doatifie\n",
      "fiveming mousts autistic the sympec a chame for inin of the sex daty lail tpilla\n",
      "hins moust in unonr stal megin heq sperenmat vion in this adree of the pive thre\n",
      "qial some in the clis two wae in they ro of betates ob in for ear lote in two in\n",
      "================================================================================\n",
      "Validation set perplexity: 7.19\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
